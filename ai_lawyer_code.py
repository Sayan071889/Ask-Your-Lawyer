# -*- coding: utf-8 -*-
"""AI Lawyer code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-zFKFJWCV2JltSzvyK4VHAON63Q8GMxW

## **Developing the Front End**
"""

!pip install pymupdf

!pip install streamlit

!wget -q -O - ipv4.icanhazip.com

! streamlit run app.py & npx localtunnel --port 8501

"""# ***---------------------------------------------------------------------------------------------***

# **Setting up of VECTOR DATABASE**
"""

!pip install langchain \
            langchain-community \
            langchain-core \
            langchain-groq \
            faiss-cpu \
            pdfplumber

!pip install langchain-huggingface

from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

"""### **Step 1: Upload & Load raw PDF(s)**

"""

pdfs_directory = 'pdfs/'

def upload_pdf(file):
    with open(pdfs_directory + file.name, "wb") as f:
        f.write(file.getbuffer())

def load_pdf(file_path):
    loader = PDFPlumberLoader(file_path)
    documents = loader.load()
    return documents

file_path = '/content/universal_declaration_of_human_rights (1).pdf'
documents = load_pdf(file_path)

"""### **Step 2: Create Chunks**"""

def create_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 1000,
        chunk_overlap = 200,
        add_start_index = True
    )
    text_chunks = text_splitter.split_documents(documents)
    return text_chunks

text_chunks = create_chunks(documents)

print("Chunks count: ", len(text_chunks))

print("PDF pages: ",len(documents))

"""### **Step 3: Setup Embeddings Model (Use HuggingFace version --- *all-MiniLM-L6-v2*)**"""

huggingface_model_name="all-MiniLM-L6-v2"

def get_embedding_model(huggingface_model_name):
    embeddings = HuggingFaceEmbeddings(model=huggingface_model_name)
    return embeddings

"""### **Step 4: Index Documents Store embeddings in FAISS (vector store)**"""

embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

faiss_db = FAISS.from_documents(text_chunks, embedding_model)
faiss_db.save_local("vectorstore/db_faiss")

"""# **-----------------------------------------------------------------------------------------------**

# **Implementing and Initializing the RAG Pipeline**
"""

from langchain_groq import ChatGroq
from vectorstore import db_faiss
from langchain_core.prompts import ChatPromptTemplate

"""###**Step1: Setup LLM (Use DeepSeek R1 with Groq)**"""

import os
os.environ["GROQ_API_KEY"] = ##"API KEY"

from langchain_groq import ChatGroq

llm_model = ChatGroq(
    model_name="deepseek-r1-distill-llama-70b",
    api_key=os.environ["GROQ_API_KEY"]
)

"""###**Step2: Retrieve Docs**"""

import sys
sys.path.append("/content/drive/MyDrive/Colab Notebooks/RAG with Hugging Face")

from vectorstore import db_faiss

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

from langchain.embeddings import HuggingFaceEmbeddings
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

faiss_db = FAISS.load_local(
    "vectorstore/db_faiss",
    embedding_model,
    allow_dangerous_deserialization=True
)

def retrieve_docs(faiss_db, query):
    return faiss_db.similarity_search(query)

def retrieve_docs(faiss_db, query):
    return faiss_db.similarity_search(query)

def get_context(documents):
    context = "\n\n".join([doc.page_content for doc in documents])
    return context

"""###**Step3: Answer Question**"""

custom_prompt_template = """
Use the pieces of information provided in the context to answer user's question.
If you dont know the answer, just say that you dont know, dont try to make up an answer.
Dont provide anything out of the given context
Question: {question}
Context: {context}
Answer:
"""

def answer_query(documents, model, query):
    context = get_context(documents)
    prompt = ChatPromptTemplate.from_template(custom_prompt_template)
    chain = prompt | model
    return chain.invoke({"question": query, "context": context})

question="If a government forbids the right to assemble peacefully which articles are violated and why?"
retrieved_docs=retrieve_docs(faiss_db, question)
print("AI Lawyer: ",answer_query(documents=retrieved_docs, model=llm_model, query=question))

print(type(faiss_db))